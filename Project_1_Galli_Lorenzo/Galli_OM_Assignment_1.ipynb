{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Methods: Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. What is the definition of a symmetric matrix?**\n",
    "\n",
    "\n",
    "\n",
    "A **symmetric matrix** is a square matrix that is equal to its **transpose**. Formally, a matrix $A$ is symmetric if:\n",
    "\n",
    "$$\n",
    "A = A^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. What are the definitions of eigenvalues and eigenvectors?**\n",
    "\n",
    "Let $V$ be a vector space over a field $K$ (for example, $\\mathbb{R}$, the field of real numbers), and let the linear application $f: V \\to V$ be an **endomorphism**. \n",
    "\n",
    "We say that a vector $v \\in V$ is an **eigenvector**, and $\\lambda$ is its **eigenvalue**, if:\n",
    "\n",
    "$$\n",
    "f(v) = \\lambda v\n",
    "$$\n",
    "\n",
    "In other words, the linear transformation **does not change the direction** of $v$, but only **scales** it by a factor of $\\lambda$.\n",
    "\n",
    "In the case of a **matrix** $A$, a vector $v$ is an eigenvector, and $\\lambda$ is its eigenvalue if:\n",
    "\n",
    "$$\n",
    "Av = \\lambda v\n",
    "$$\n",
    "\n",
    "This means that applying the transformation $A$ to $v$ is the same as scaling $v$ by $\\lambda$.\n",
    "\n",
    "Most vectors change direction when a transformation (such as rotation or stretching) is applied. However, **eigenvectors** are \"special\" because they only get stay stretched, squished or flipped but they always stay on the same line, without changing direction.\n",
    "\n",
    "A matrix may have multiple eigenvectors with different eigenvalues. If we find a complete set of eigenvectors that span the space, they form an **eigenbasis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. What is a singular matrix?**\n",
    "\n",
    "A **singular matrix** is a square matrix that **doesn't have an inverse**. This occurs when the determinant of the matrix is zero. Formally, a matrix $A$ is **singular** if:\n",
    "\n",
    "$$\n",
    "\\det(A) = 0\n",
    "$$\n",
    "\n",
    "In fact, a singular matrix has **linearly dependent** rows or columns. This means that at least one row or column can be written as a linear combination of others. Consequently, The **rank** of a singular matrix is less than its size. A singular matrix always has at least one **eigenvalue** equal to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. What is the definition of a positive definite matrix? definition of a positive semi-definite matrix?**\n",
    "\n",
    "A **square matrix** $A$ is **positive definite** if, for every non-zero vector $x \\in \\mathbb{R}^n$, it’s true that:\n",
    "\n",
    "$$\n",
    "x^T A x > 0\n",
    "$$\n",
    "\n",
    "In particular, a positive definite matrix is always **symmetric**, which means $A = A^T$, but the opposite is not guaranteed. Furthermore, in a positive definite matrix, all eigenvalues of $A$ are strictly positive ($\\lambda_i > 0$), and the determinants of all sub-matrices obtained by eliminating successive rows and columns are positive.\n",
    "\n",
    "### Example:\n",
    "\n",
    "I will use an example I created in the Numerical Computing course. Let’s verify if the following matrix is positive definite:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "2 & -1 \\\\\n",
    "-1 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We will use a generic vector $x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$.\n",
    "\n",
    "We want to compute:\n",
    "\n",
    "$$\n",
    "x^T A x = \n",
    "\\begin{bmatrix} x_1 & x_2 \\end{bmatrix}\n",
    "\\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We first compute $A x$:\n",
    "\n",
    "$$\n",
    "A x =\n",
    "\\begin{bmatrix}\n",
    "2 & -1 \\\\\n",
    "-1 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2x_1 - x_2 \\\\\n",
    "-x_1 + 2x_2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, we compute $x^T A x$:\n",
    "\n",
    "$$\n",
    "x^T A x = \n",
    "\\begin{bmatrix} x_1 & x_2 \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "2x_1 - x_2 \\\\\n",
    "-x_1 + 2x_2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x^T A x = x_1(2x_1 - x_2) + x_2(-x_1 + 2x_2)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "x^T A x = 2x_1^2 - x_1 x_2 - x_1 x_2 + 2x_2^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "x^T A x = 2x_1^2 - 2x_1 x_2 + 2x_2^2\n",
    "$$\n",
    "\n",
    "This can be rewritten as:\n",
    "\n",
    "$$\n",
    "x^T A x = (x_1 - x_2)^2 + x_1^2 + x_2^2\n",
    "$$\n",
    "\n",
    "Since $(x_1 - x_2)^2$, $x_1^2$, and $x_2^2$ are all non-negative, and their sum is strictly positive for any nonzero vector $x$, we conclude that the matrix $A$ is **positive definite**.\n",
    "\n",
    "Similarly, A **positive semi-definite matrix** is a symmetric matrix $A$ that satisfies the following condition for all vectors $x \\in \\mathbb{R}^n$:\n",
    "\n",
    "$$\n",
    "x^T A x \\geq 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Let $M \\in \\mathbb{R}^{n \\times n}$ a nonsingular square matrix and let $A = M^T M$. Prove that $A$ is positive definite. Hint: Recall that $x^T x = \\|x\\|^2$ for $x \\in \\mathbb{R}^n$.**\n",
    "\n",
    "To prove that $A = M^T M$ is positive definite, we need to show that for every non-zero vector $x \\in \\mathbb{R}^n$:\n",
    "\n",
    "$$\n",
    "x^T A x > 0\n",
    "$$\n",
    "\n",
    "Since\n",
    "\n",
    "$$\n",
    "A = M^T M\n",
    "$$\n",
    "\n",
    "We consider the expression $x^T A x$ as:\n",
    "\n",
    "$$\n",
    "x^T A x = x^T (M^T M) x\n",
    "$$\n",
    "\n",
    "And we can rewrite this expression as:\n",
    "\n",
    "$$\n",
    "x^T (M^T M) x = (Mx)^T (Mx)\n",
    "$$\n",
    "\n",
    "The term $(Mx)^T (Mx)$ is the dot product of the vector $Mx$ with itself, which is the square of the Euclidean norm of the vector $Mx$:\n",
    "\n",
    "$$\n",
    "(Mx)^T (Mx) = \\|Mx\\|^2\n",
    "$$\n",
    "\n",
    "So, we have:\n",
    "\n",
    "$$\n",
    "x^T A x = \\|Mx\\|^2\n",
    "$$\n",
    "\n",
    "Since $M$ is nonsingular (invertible), then there is no non-zero vector $x$ such that $M x = 0$ and:\n",
    "\n",
    "$$\n",
    "\\|Mx\\|^2 > 0\n",
    "$$\n",
    "\n",
    "So, for any non-zero $x \\in \\mathbb{R}^n$:\n",
    "\n",
    "$$\n",
    "x^T A x = \\|Mx\\|^2 > 0\n",
    "$$\n",
    "\n",
    "We can conclude that, since $x^T A x > 0$ for all non-zero vectors $x \\in \\mathbb{R}^n$, $A = M^T M$ is **positive definite**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: programming problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Create a vector b with the elements:**\n",
    "\n",
    "$$\n",
    "b = \\begin{bmatrix}\n",
    "-3 \\\\\n",
    "-8 \\\\\n",
    "7\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3 -8  7]\n"
     ]
    }
   ],
   "source": [
    "# Create vector B\n",
    "b = np.array([-3, -8, 7])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Create and print the matrix $A$ with the elements:**\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "-10 & 3 & 11 \\\\\n",
    "3 & -5 & -4 \\\\\n",
    "11 & -4 & -7\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10   3  11]\n",
      " [  3  -5  -4]\n",
      " [ 11  -4  -7]]\n"
     ]
    }
   ],
   "source": [
    "# Create matrix A\n",
    "A = np.array([[-10, 3, 11],\n",
    "              [3, -5, -4],\n",
    "              [11, -4, -7]])\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Is the matrix $A$ symmetric? Write your answer in a cell in the notebook and then check it with Python.**\n",
    "\n",
    "Yes the matrix $A$ is symmetric, and to show that we can easily check that the matrix $A$ is symmetric by comparing it to its transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix A is symmetric.\n"
     ]
    }
   ],
   "source": [
    "# Check if the matrix A is symmetric, meaning if it's equal to its transpose\n",
    "symmetric = np.array_equal(A, A.T)\n",
    "\n",
    "if symmetric:\n",
    "    print(\"The matrix A is symmetric.\")\n",
    "else:\n",
    "    print(\"The matrix A is not symmetric.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Compute $x$ as the solution of the linear system $A x = b$ and print it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vector b is: [-3 -8  7]\n",
      "The solution vector x is: [2. 2. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"The vector b is:\", b)\n",
    "\n",
    "# Solve for x in the equation Ax = b\n",
    "x = np.linalg.solve(A, b)\n",
    "print(\"The solution vector x is:\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. To check if the solution is correct, compute $r$ as the product $A x$ and print it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The residual r = Ax is: [-3. -8.  7.]\n"
     ]
    }
   ],
   "source": [
    "# Compute the residual r = Ax to check if the solution is correct (the values should match the values of vector b)\n",
    "r = np.dot(A, x)\n",
    "print(\"The residual r = Ax is:\", r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Print the difference of $r$ and $b$ as $||r−b||$. (This should be zero. Is it? Briefly discuss this in the cell for this question.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference (r - b): [ 1.77635684e-15  8.88178420e-16 -1.77635684e-15]\n",
      "Euclidean norm of (r - b): 2.6645352591003757e-15\n",
      "The solution is correct (approximately zero).\n"
     ]
    }
   ],
   "source": [
    "# Compute the difference r - b\n",
    "diff = r - b\n",
    "\n",
    "# Compute the Euclidean norm of the difference ||r - b||\n",
    "norm_diff = np.linalg.norm(diff)\n",
    "\n",
    "print(\"Difference (r - b):\", diff)\n",
    "print(\"Euclidean norm of (r - b):\", norm_diff)\n",
    "\n",
    "tolerance = 1e-10\n",
    "if norm_diff < tolerance:\n",
    "    print(\"The solution is correct (approximately zero).\")\n",
    "else:\n",
    "    print(\"The solution is not correct because has a non-zero residual.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the Euclidean norm $||r - b||$ to be 0 because it represents the residual error between the result $r$ (calculated as $A x$) and the vector $b$ (the expected result). If the norm is 0, it means that our solution $x$ perfectly satisfies the equation $A x = b$ with no errors.\n",
    "\n",
    "However, when solving a system of linear equations using $np.linalg.solve(A, b)$, the computed solution $x$ is represented in floating-point format. When we work with floating-point numbers in computers, we often have some small rounding errors. This happens because many numbers cannot be represented exactly in binary form. This limitation, like in this case, is the main reason why floating-point calculations can sometimes give results that are very close to the correct value, but not exactly the same. For instance, the result we just got is a very small number, and it is essentially zero, but we still get a small error. When we work with floating-point numbers we often treat very small numbers as zero, usually setting a small threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Print the determinant of $A$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The determinant of A is: 214.0000000000001\n"
     ]
    }
   ],
   "source": [
    "# Calculate the determinant of A\n",
    "det_A = np.linalg.det(A)\n",
    "print(\"The determinant of A is:\", det_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. Repeat questions 2-7 for the matrix $A1$ with the elements:**\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "-1 & 3000 & 1 \\\\\n",
    "2000 & -1.5 & -2.5 \\\\\n",
    "-1999 & -2998.5 & 1.5\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it's not a good thing to duplicate the code, in order to show everything clearly I will copy the code in the box below to show the results for this new matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   -1.   3000.      1. ]\n",
      " [ 2000.     -1.5    -2.5]\n",
      " [-1999.  -2998.5     1.5]]\n",
      "The matrix A is not symmetric.\n",
      "The vector b is: [-3 -8  7]\n",
      "The solution vector x is: [ 2.2514e+13 -5.9973e+12  1.8014e+16]\n",
      "The residual r = Ax is: [-8. -8. 16.]\n",
      "Difference (r - b): [-5.  0.  9.]\n",
      "Euclidean norm of (r - b): 10.295630140987\n",
      "The solution is not correct because has a non-zero residual.\n",
      "The determinant of A is: 6.6613364824164e-10\n",
      "Condition number of A1: 2.857550790184196e+18\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "A1 = np.array([[-1, 3000, 1],\n",
    "              [2000, -1.5, -2.5],\n",
    "              [-1999, -2998.5, 1.5]])\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "print(A1)\n",
    "\n",
    "# 3\n",
    "symmetric = np.array_equal(A1, A1.T)\n",
    "\n",
    "if symmetric:\n",
    "    print(\"The matrix A is symmetric.\")\n",
    "else:\n",
    "    print(\"The matrix A is not symmetric.\")\n",
    "\n",
    "# 4\n",
    "x1 = np.linalg.solve(A1, b)\n",
    "print(\"The vector b is:\", b)\n",
    "print(\"The solution vector x is:\", x1)\n",
    "\n",
    "# 5\n",
    "r1 = np.dot(A1, x1)\n",
    "print(\"The residual r = Ax is:\", r1)\n",
    "\n",
    "# 6\n",
    "diff1 = r1 - b\n",
    "norm_diff1 = np.linalg.norm(diff1)\n",
    "print(\"Difference (r - b):\", diff1)\n",
    "print(\"Euclidean norm of (r - b):\", norm_diff1)\n",
    "\n",
    "tolerance = 1e-10\n",
    "if norm_diff1 < tolerance:\n",
    "    print(\"The solution is correct (approximately zero).\")\n",
    "else:\n",
    "    print(\"The solution is not correct because has a non-zero residual.\")\n",
    "    \n",
    "# 7\n",
    "det_A1 = np.linalg.det(A1)\n",
    "print(\"The determinant of A is:\", det_A1)\n",
    "\n",
    "# Condition number\n",
    "cond_A1 = np.linalg.cond(A1)\n",
    "print(\"Condition number of A1:\", cond_A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are reported below:\n",
    "\n",
    "2. I created the matrix $A1$\n",
    "\n",
    "3. The matrix $A1$ is not symmetric\n",
    "\n",
    "4. With the vector $b$ = [-3, -8, 7] the solution vector $x$ is: [2.2514e+13, -5.9973e+12, 1.8014e+16]\n",
    "\n",
    "5. The residual $r = A x$ is: [-8, -8, 16]\n",
    "\n",
    "6. The Difference $r - b$ = [-5, 0, 9] and the Euclidean norm is 10.295630140987, meaning the solution has a non-zero residual so it's not accurate.\n",
    "\n",
    "7. The determinant of matrix $A1$ is: 6.6613364824164e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for this matrix $A_1$ are \"weird\" but they can be explained by considering the numerical instability given bt the high condition number and the determinant of the matrix really close to 0.\n",
    "\n",
    "First of all, as we said, the residual vector $r - b$ ideally should be close to zero, indicating that the computed solution accurately satisfies $A x = b$.\n",
    "\n",
    "However, in this case, the Euclidean norm of the residual $\\|r - b\\| \\approx 10.2956$ is relatively large, suggesting that we encountered some numerical unprecisions.\n",
    "\n",
    "In fact, the solution vector contains values on the order of $10^{12}$ to $10^{16}$, which can lead to numerical instability. In particular, extremely large solution values typically occur when the matrix $A_1$ is nearly singular or ill-conditioned.\n",
    "\n",
    "We notice that the determinant of $A_1$ is extremely small: $\\text{det}(A_1) \\approx 6.66 \\times 10^{-10}$ and, like we saw in part 1, a determinant very close to zero implies that the matrix is nearly singular, making it numerically difficult to solve the system $A x = b$.\n",
    "\n",
    "However, in this case the main problem is given by the fact that the matrix is ill-conditioned. Since the condition number is given by the difference of the largest eigenvalue for the smallest eigenvalue, a condition number of $2.85 \\times 10^{18}$ means that the largest eigenvalue is much bigger than the smallest eigenvalue, confirming that the matrix is nearly singular. All this means that even small changes in $b$ or small floating point numbers errors in rounding can significantly impact the solution $x$, leading to numerical instability. This basically makes $np.linalg.solve(A1,b)$ unreliable for this case. Maybe, we should instead choose another method more efficient for this kind of matrices, for example using the Singular Value Decomposition (SVD) for a more stable solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Methods SP25 - Assignment 1\n",
      "Lorenzo Galli - Università della Svizzera Italiana (USI)\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimization Methods SP25 - Assignment 1\")\n",
    "print(\"Lorenzo Galli - Università della Svizzera Italiana (USI)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
